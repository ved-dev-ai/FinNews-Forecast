{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation and numerical operations\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Technology stocks dataset\n",
    "df = pd.read_csv(\"../Datasets/category_csvs/Technology_stocks_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce6305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 47362\n",
      "After filtering: 47122\n",
      "\n",
      "Class distribution:\n",
      "Label\n",
      "1    25794\n",
      "0    21328\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LABEL GENERATION: Predict if majority of next 5 days will be UP days\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort by stock symbol and date to ensure proper time series ordering\n",
    "df = df.sort_values(['Stock_symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Step 1: Compute daily returns for each stock\n",
    "df['Daily_Return'] = df.groupby('Stock_symbol')['Close'].pct_change()\n",
    "\n",
    "# Step 2: Mark up/down days (1 = up day, 0 = down or flat day)\n",
    "df['Up_Day'] = (df['Daily_Return'] > 0).astype(int)\n",
    "\n",
    "# Step 3: Count UP days in the next 5 trading days (forward-looking window)\n",
    "df['Up_Count_5d'] = df.groupby('Stock_symbol')['Up_Day'].transform(\n",
    "    lambda x: x.shift(-1).rolling(window=5, min_periods=5).sum()\n",
    ")\n",
    "\n",
    "# Step 4: Create binary label - majority vote (>=3 up days = UP trend)\n",
    "df['Label'] = np.where(df['Up_Count_5d'] >= 3, 1, 0)\n",
    "\n",
    "# Step 5: Remove rows without a full 5-day forward window (end of data)\n",
    "df_filtered = df.dropna(subset=['Up_Count_5d', 'Label'])\n",
    "\n",
    "print(f\"Original samples: {len(df)}\")\n",
    "print(f\"After filtering: {len(df_filtered)}\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_filtered['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17413365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by stock symbol and date for time series operations\n",
    "df = df.sort_values([\"Stock_symbol\", \"Date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7736441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MOMENTUM FEATURES: Short, medium, and long-term price returns\n",
    "# ============================================================================\n",
    "\n",
    "# Group by stock symbol for per-stock calculations\n",
    "g = df.groupby(\"Stock_symbol\")\n",
    "\n",
    "# Calculate percentage returns over different time windows\n",
    "df[\"Return_1d\"]  = g[\"Adj Close\"].pct_change(1)   # 1-day return\n",
    "df[\"Return_5d\"]  = g[\"Adj Close\"].pct_change(5)   # 5-day return (weekly)\n",
    "df[\"Return_20d\"] = g[\"Adj Close\"].pct_change(20)  # 20-day return (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9386f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MOVING AVERAGE FEATURES: Trend indicators\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate moving averages\n",
    "df[\"MA5\"]  = g[\"Adj Close\"].transform(lambda x: x.rolling(5).mean())   # 5-day MA\n",
    "df[\"MA20\"] = g[\"Adj Close\"].transform(lambda x: x.rolling(20).mean())  # 20-day MA\n",
    "\n",
    "# Calculate price relative to moving averages (>1 means above MA)\n",
    "df[\"MA5_Ratio\"]  = df[\"Adj Close\"] / df[\"MA5\"]\n",
    "df[\"MA20_Ratio\"] = df[\"Adj Close\"] / df[\"MA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a74505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TREND FEATURE: Relationship between short and long-term moving averages\n",
    "# ============================================================================\n",
    "\n",
    "# Positive values indicate short-term MA above long-term MA (bullish)\n",
    "df[\"Trend_5_20\"] = (df[\"MA5\"] - df[\"MA20\"]) / df[\"MA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISTANCE FROM EXTREMES: Position relative to recent highs/lows\n",
    "# ============================================================================\n",
    "\n",
    "# Get 20-day high and low values\n",
    "df[\"High_20\"] = g[\"High\"].transform(lambda x: x.rolling(20).max())\n",
    "df[\"Low_20\"]  = g[\"Low\"].transform(lambda x: x.rolling(20).min())\n",
    "\n",
    "# Calculate distance from extremes (negative = below high/low)\n",
    "df[\"Dist_20High\"] = (df[\"Adj Close\"] - df[\"High_20\"]) / df[\"High_20\"]\n",
    "df[\"Dist_20Low\"]  = (df[\"Adj Close\"] - df[\"Low_20\"]) / df[\"Low_20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437976b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY FEATURES: Short and medium-term price volatility\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate rolling standard deviation of returns (volatility measure)\n",
    "df[\"Vol_5d\"]  = g[\"Return_1d\"].transform(lambda x: x.rolling(5).std())   # 5-day volatility\n",
    "df[\"Vol_20d\"] = g[\"Return_1d\"].transform(lambda x: x.rolling(20).std())  # 20-day volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY RATIO: Recent vs historical volatility\n",
    "# ============================================================================\n",
    "\n",
    "# Ratio > 1 indicates increasing volatility\n",
    "df[\"Vol_Ratio\"] = df[\"Vol_5d\"] / df[\"Vol_20d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54604be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GAP FEATURE: Overnight price gap\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate gap between today's open and yesterday's close\n",
    "prev_close = g[\"Adj Close\"].shift(1)\n",
    "df[\"Gap\"] = (df[\"Open\"] - prev_close) / prev_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLUME CHANGE: Day-over-day volume change\n",
    "# ============================================================================\n",
    "\n",
    "# Daily percentage change in trading volume\n",
    "df[\"Vol_Change\"] = g[\"Volume\"].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLUME RATIO: Current volume relative to 20-day average\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate 20-day average volume\n",
    "df[\"Vol_MA20\"] = g[\"Volume\"].transform(lambda x: x.rolling(20).mean())\n",
    "\n",
    "# Ratio > 1 indicates above-average volume\n",
    "df[\"Vol_Ratio_20\"] = df[\"Volume\"] / df[\"Vol_MA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fa88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRICE-VOLUME SCORE: Combined price and volume momentum\n",
    "# ============================================================================\n",
    "\n",
    "# Positive score = price up with high volume (strong signal)\n",
    "df[\"PV_Score\"] = df[\"Return_1d\"] * df[\"Vol_Ratio_20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLUME TREND: Short-term vs medium-term volume trend\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate 5-day average volume\n",
    "df[\"Vol_MA5\"] = g[\"Volume\"].transform(lambda x: x.rolling(5).mean())\n",
    "\n",
    "# Ratio > 1 indicates increasing volume trend\n",
    "df[\"Vol_Trend\"] = df[\"Vol_MA5\"] / df[\"Vol_MA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NET SENTIMENT: Difference between positive and negative sentiment\n",
    "# ============================================================================\n",
    "\n",
    "# Net sentiment score (positive - negative article sentiment)\n",
    "df[\"Sent_Net\"] = df[\"positive\"] - df[\"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SENTIMENT MOVING AVERAGE: Smoothed sentiment trend\n",
    "# ============================================================================\n",
    "\n",
    "# 5-day moving average of net sentiment\n",
    "df[\"Sent_MA5\"] = g[\"Sent_Net\"].transform(\n",
    "    lambda x: x.rolling(5).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SENTIMENT MOMENTUM: Change in sentiment over 5 days\n",
    "# ============================================================================\n",
    "\n",
    "# Positive = improving sentiment\n",
    "df[\"Sent_Mom5\"] = g[\"Sent_Net\"].diff(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499440d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEWS INTENSITY: Current news coverage relative to average\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate 20-day average article count\n",
    "df[\"Art_MA20\"] = g[\"article_count\"].transform(\n",
    "    lambda x: x.rolling(20).mean()\n",
    ")\n",
    "\n",
    "# Ratio > 1 indicates higher than average news coverage\n",
    "df[\"News_Intensity\"] = df[\"article_count\"] / df[\"Art_MA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LAGGED FEATURES: Historical values to capture delayed effects\n",
    "# ============================================================================\n",
    "\n",
    "# Lagged return and sentiment features\n",
    "df[\"Ret_Lag1\"]  = g[\"Return_1d\"].shift(1)   # Yesterday's return\n",
    "df[\"Ret_Lag5\"]  = g[\"Return_5d\"].shift(5)   # Return from 5 days ago\n",
    "df[\"Sent_Lag3\"] = g[\"Sent_Net\"].shift(3)    # Sentiment from 3 days ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d90c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE LIST: All engineered features for the model\n",
    "# ============================================================================\n",
    "\n",
    "features = [\n",
    "    # Momentum features - capture price trends\n",
    "    \"Return_1d\",\"Return_5d\",\"Return_20d\",\n",
    "    \"MA5_Ratio\",\"MA20_Ratio\",\"Trend_5_20\",\n",
    "    \"Dist_20High\",\"Dist_20Low\",\n",
    "\n",
    "    # Volatility features - capture price stability/risk\n",
    "    \"Vol_5d\",\"Vol_20d\",\n",
    "    \"Vol_Ratio\",\"Gap\",\n",
    "\n",
    "    # Volume features - capture trading activity\n",
    "    \"Vol_Change\",\"Vol_Ratio_20\",\"PV_Score\",\"Vol_Trend\",\n",
    "\n",
    "    # Sentiment features - capture market sentiment from news\n",
    "    \"Sent_Net\",\"Sent_MA5\",\"Sent_Mom5\",\n",
    "    \"News_Intensity\",\n",
    "\n",
    "    # Lagged features - capture historical effects\n",
    "    \"Ret_Lag1\",\"Ret_Lag5\",\"Sent_Lag3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38607be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return_1d</th>\n",
       "      <th>Return_5d</th>\n",
       "      <th>Return_20d</th>\n",
       "      <th>MA5_Ratio</th>\n",
       "      <th>MA20_Ratio</th>\n",
       "      <th>Trend_5_20</th>\n",
       "      <th>Dist_20High</th>\n",
       "      <th>Dist_20Low</th>\n",
       "      <th>Vol_5d</th>\n",
       "      <th>Vol_20d</th>\n",
       "      <th>...</th>\n",
       "      <th>Vol_Ratio_20</th>\n",
       "      <th>PV_Score</th>\n",
       "      <th>Vol_Trend</th>\n",
       "      <th>Sent_Net</th>\n",
       "      <th>Sent_MA5</th>\n",
       "      <th>Sent_Mom5</th>\n",
       "      <th>News_Intensity</th>\n",
       "      <th>Ret_Lag1</th>\n",
       "      <th>Ret_Lag5</th>\n",
       "      <th>Sent_Lag3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.190918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.080374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.190918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.213160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.030031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.080374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47357</th>\n",
       "      <td>0.014143</td>\n",
       "      <td>-0.025169</td>\n",
       "      <td>0.174666</td>\n",
       "      <td>0.995012</td>\n",
       "      <td>1.079556</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.053580</td>\n",
       "      <td>0.174822</td>\n",
       "      <td>0.017633</td>\n",
       "      <td>0.028121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671484</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>1.232345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017359</td>\n",
       "      <td>-0.228216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028988</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47358</th>\n",
       "      <td>0.015454</td>\n",
       "      <td>-0.002850</td>\n",
       "      <td>0.176874</td>\n",
       "      <td>1.010972</td>\n",
       "      <td>1.087282</td>\n",
       "      <td>0.075481</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>0.183440</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.028142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534997</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.849365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47359</th>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.033073</td>\n",
       "      <td>0.190065</td>\n",
       "      <td>1.027027</td>\n",
       "      <td>1.102075</td>\n",
       "      <td>0.073073</td>\n",
       "      <td>-0.017230</td>\n",
       "      <td>0.199652</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540786</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>0.751300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015454</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47360</th>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.023643</td>\n",
       "      <td>0.179144</td>\n",
       "      <td>1.023248</td>\n",
       "      <td>1.094066</td>\n",
       "      <td>0.069209</td>\n",
       "      <td>-0.016195</td>\n",
       "      <td>0.187266</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690363</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.669530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.125491</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47361</th>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>0.163680</td>\n",
       "      <td>1.011305</td>\n",
       "      <td>1.083993</td>\n",
       "      <td>0.071876</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>0.185372</td>\n",
       "      <td>0.010244</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496664</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>0.613722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.070286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47362 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Return_1d  Return_5d  Return_20d  MA5_Ratio  MA20_Ratio  Trend_5_20  \\\n",
       "0            NaN        NaN         NaN        NaN         NaN         NaN   \n",
       "1       0.080029        NaN         NaN        NaN         NaN         NaN   \n",
       "2      -0.190918        NaN         NaN        NaN         NaN         NaN   \n",
       "3      -0.080374        NaN         NaN        NaN         NaN         NaN   \n",
       "4       0.213160        NaN         NaN   1.030031         NaN         NaN   \n",
       "...          ...        ...         ...        ...         ...         ...   \n",
       "47357   0.014143  -0.025169    0.174666   0.995012    1.079556    0.084968   \n",
       "47358   0.015454  -0.002850    0.176874   1.010972    1.087282    0.075481   \n",
       "47359   0.022605   0.033073    0.190065   1.027027    1.102075    0.073073   \n",
       "47360   0.001053   0.023643    0.179144   1.023248    1.094066    0.069209   \n",
       "47361  -0.001595   0.052521    0.163680   1.011305    1.083993    0.071876   \n",
       "\n",
       "       Dist_20High  Dist_20Low    Vol_5d   Vol_20d  ...  Vol_Ratio_20  \\\n",
       "0              NaN         NaN       NaN       NaN  ...           NaN   \n",
       "1              NaN         NaN       NaN       NaN  ...           NaN   \n",
       "2              NaN         NaN       NaN       NaN  ...           NaN   \n",
       "3              NaN         NaN       NaN       NaN  ...           NaN   \n",
       "4              NaN         NaN       NaN       NaN  ...           NaN   \n",
       "...            ...         ...       ...       ...  ...           ...   \n",
       "47357    -0.053580    0.174822  0.017633  0.028121  ...      0.671484   \n",
       "47358    -0.038954    0.183440  0.019696  0.028142  ...      0.534997   \n",
       "47359    -0.017230    0.199652  0.020442  0.028314  ...      0.540786   \n",
       "47360    -0.016195    0.187266  0.020454  0.028368  ...      0.690363   \n",
       "47361    -0.017765    0.185372  0.010244  0.028449  ...      0.496664   \n",
       "\n",
       "       PV_Score  Vol_Trend  Sent_Net  Sent_MA5  Sent_Mom5  News_Intensity  \\\n",
       "0           NaN        NaN       0.0       NaN        NaN             NaN   \n",
       "1           NaN        NaN       0.0       NaN        NaN             NaN   \n",
       "2           NaN        NaN       0.0       NaN        NaN             NaN   \n",
       "3           NaN        NaN       0.0       NaN        NaN             NaN   \n",
       "4           NaN        NaN       0.0  0.000000        NaN             NaN   \n",
       "...         ...        ...       ...       ...        ...             ...   \n",
       "47357  0.009497   1.232345       0.0 -0.017359  -0.228216             0.0   \n",
       "47358  0.008268   0.849365       0.0  0.000000   0.086795             0.0   \n",
       "47359  0.012224   0.751300       0.0  0.000000   0.000000             0.0   \n",
       "47360  0.000727   0.669530       0.0  0.000000   0.000000             0.0   \n",
       "47361 -0.000792   0.613722       0.0  0.000000   0.000000             0.0   \n",
       "\n",
       "       Ret_Lag1  Ret_Lag5  Sent_Lag3  \n",
       "0           NaN       NaN        NaN  \n",
       "1           NaN       NaN        NaN  \n",
       "2      0.080029       NaN        NaN  \n",
       "3     -0.190918       NaN        0.0  \n",
       "4     -0.080374       NaN        0.0  \n",
       "...         ...       ...        ...  \n",
       "47357 -0.028988  0.166917        0.0  \n",
       "47358  0.014143  0.144200        0.0  \n",
       "47359  0.015454  0.112789        0.0  \n",
       "47360  0.022605  0.125491        0.0  \n",
       "47361  0.001053  0.070286        0.0  \n",
       "\n",
       "[47362 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the raw features (before normalization)\n",
    "df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE NORMALIZATION: Z-score normalization using rolling window\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use 252-day window (approximately 1 trading year)\n",
    "WINDOW = 252\n",
    "\n",
    "# Normalize each feature using rolling mean and standard deviation\n",
    "for f in features:\n",
    "    # Calculate rolling statistics per stock\n",
    "    rolling_mean = g[f].transform(\n",
    "        lambda x: x.rolling(WINDOW).mean()\n",
    "    )\n",
    "    \n",
    "    rolling_std = g[f].transform(\n",
    "        lambda x: x.rolling(WINDOW).std()\n",
    "    )\n",
    "    \n",
    "    # Z-score normalization: (value - mean) / std\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    df[f + \"_z\"] = (df[f] - rolling_mean) / (rolling_std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87960491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of normalized feature names (all ending with '_z')\n",
    "final_features = [f + \"_z\" for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in features or labels\n",
    "df_model = df.dropna(subset=final_features + [\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13522f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT ML LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,        # Area under ROC curve\n",
    "    accuracy_score,       # Overall accuracy\n",
    "    precision_score,      # Precision (TP / (TP + FP))\n",
    "    recall_score,         # Recall (TP / (TP + FN))\n",
    "    f1_score,            # Harmonic mean of precision and recall\n",
    "    log_loss,            # Logarithmic loss\n",
    "    classification_report # Detailed classification metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (all normalized features) and target variable\n",
    "FEATURES = [c for c in df_model.columns if c.endswith(\"_z\")]\n",
    "TARGET = \"Label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f995bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR TIME SERIES WALK-FORWARD VALIDATION\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a copy and ensure Date column is datetime type\n",
    "df_model = df_model.copy()\n",
    "df_model[\"Date\"] = pd.to_datetime(df_model[\"Date\"])\n",
    "\n",
    "# Extract all unique months for potential monthly splits\n",
    "months = (\n",
    "    df_model[\"Date\"]\n",
    "    .dt.to_period(\"M\")\n",
    "    .drop_duplicates()\n",
    "    .sort_values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL PIPELINE: XGBoost Classifier with optimized hyperparameters\n",
    "# ============================================================================\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def make_pipeline():\n",
    "    \"\"\"\n",
    "    Create XGBoost pipeline with tuned hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline: Scikit-learn pipeline with XGBoost classifier\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"model\", XGBClassifier(\n",
    "            n_estimators=400,        # Number of boosting rounds\n",
    "            max_depth=5,             # Maximum tree depth\n",
    "            learning_rate=0.03,      # Step size shrinkage\n",
    "            subsample=0.8,           # Fraction of samples for each tree\n",
    "            colsample_bytree=0.8,    # Fraction of features for each tree\n",
    "            eval_metric=\"auc\",       # Evaluation metric\n",
    "            random_state=42,         # For reproducibility\n",
    "            n_jobs=-1                # Use all CPU cores\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d89a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 00:00:00 | AUC: 0.735 | ACC: 0.663 | Precision: 0.654 | Recall: 0.682 | F1: 0.668 | Log Loss: 0.600\n",
      "2022-03-01 00:00:00 | AUC: 0.796 | ACC: 0.711 | Precision: 0.727 | Recall: 0.787 | F1: 0.756 | Log Loss: 0.537\n",
      "2022-03-30 00:00:00 | AUC: 0.743 | ACC: 0.686 | Precision: 0.471 | Recall: 0.608 | F1: 0.531 | Log Loss: 0.580\n",
      "2022-04-29 00:00:00 | AUC: 0.761 | ACC: 0.682 | Precision: 0.720 | Recall: 0.632 | F1: 0.673 | Log Loss: 0.590\n",
      "2022-05-31 00:00:00 | AUC: 0.749 | ACC: 0.667 | Precision: 0.616 | Recall: 0.693 | F1: 0.652 | Log Loss: 0.597\n",
      "2022-06-30 00:00:00 | AUC: 0.838 | ACC: 0.773 | Precision: 0.798 | Recall: 0.837 | F1: 0.817 | Log Loss: 0.490\n",
      "2022-08-01 00:00:00 | AUC: 0.801 | ACC: 0.702 | Precision: 0.641 | Recall: 0.839 | F1: 0.727 | Log Loss: 0.567\n",
      "2022-08-30 00:00:00 | AUC: 0.828 | ACC: 0.754 | Precision: 0.560 | Recall: 0.688 | F1: 0.617 | Log Loss: 0.491\n",
      "2022-09-29 00:00:00 | AUC: 0.811 | ACC: 0.725 | Precision: 0.778 | Recall: 0.751 | F1: 0.764 | Log Loss: 0.518\n",
      "2022-10-28 00:00:00 | AUC: 0.802 | ACC: 0.728 | Precision: 0.732 | Recall: 0.802 | F1: 0.765 | Log Loss: 0.537\n",
      "2022-11-29 00:00:00 | AUC: 0.741 | ACC: 0.714 | Precision: 0.447 | Recall: 0.613 | F1: 0.517 | Log Loss: 0.574\n",
      "2022-12-29 00:00:00 | AUC: 0.711 | ACC: 0.695 | Precision: 0.784 | Recall: 0.763 | F1: 0.773 | Log Loss: 0.587\n",
      "2023-01-31 00:00:00 | AUC: 0.737 | ACC: 0.677 | Precision: 0.693 | Recall: 0.672 | F1: 0.682 | Log Loss: 0.602\n",
      "2023-03-02 00:00:00 | AUC: 0.743 | ACC: 0.673 | Precision: 0.722 | Recall: 0.680 | F1: 0.700 | Log Loss: 0.603\n",
      "2023-03-31 00:00:00 | AUC: 0.803 | ACC: 0.724 | Precision: 0.670 | Recall: 0.727 | F1: 0.698 | Log Loss: 0.534\n",
      "2023-05-02 00:00:00 | AUC: 0.787 | ACC: 0.722 | Precision: 0.767 | Recall: 0.764 | F1: 0.766 | Log Loss: 0.552\n",
      "2023-06-01 00:00:00 | AUC: 0.767 | ACC: 0.676 | Precision: 0.798 | Recall: 0.635 | F1: 0.707 | Log Loss: 0.588\n",
      "2023-07-03 00:00:00 | AUC: 0.760 | ACC: 0.679 | Precision: 0.787 | Recall: 0.678 | F1: 0.729 | Log Loss: 0.577\n",
      "2023-08-02 00:00:00 | AUC: 0.768 | ACC: 0.680 | Precision: 0.762 | Recall: 0.611 | F1: 0.678 | Log Loss: 0.595\n",
      "2023-08-31 00:00:00 | AUC: 0.813 | ACC: 0.728 | Precision: 0.709 | Recall: 0.714 | F1: 0.712 | Log Loss: 0.515\n",
      "2023-10-02 00:00:00 | AUC: 0.750 | ACC: 0.675 | Precision: 0.616 | Recall: 0.641 | F1: 0.628 | Log Loss: 0.583\n",
      "2023-10-31 00:00:00 | AUC: 0.714 | ACC: 0.731 | Precision: 0.870 | Recall: 0.793 | F1: 0.830 | Log Loss: 0.515\n",
      "2023-11-30 00:00:00 | AUC: 0.767 | ACC: 0.727 | Precision: 0.815 | Recall: 0.787 | F1: 0.801 | Log Loss: 0.533\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WALK-FORWARD VALIDATION: Train on past year, test on next month\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_DAYS = 252      # ~1 year of training data\n",
    "TEST_DAYS  = 21       # ~1 month of testing data\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get sorted unique dates\n",
    "dates = df_model[\"Date\"].sort_values().unique()\n",
    "\n",
    "# Walk forward through time\n",
    "for i in range(TRAIN_DAYS, len(dates), TEST_DAYS):\n",
    "    # Define training period (1 year lookback)\n",
    "    train_start = dates[i - TRAIN_DAYS]\n",
    "    train_end   = dates[i - 1]\n",
    "    \n",
    "    # Define test period (1 month forward)\n",
    "    test_start  = dates[i]\n",
    "    test_end    = dates[min(i + TEST_DAYS - 1, len(dates)-1)]\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Split data into train and test sets\n",
    "    # ----------------------------\n",
    "    train = df_model[\n",
    "        (df_model[\"Date\"] >= train_start) &\n",
    "        (df_model[\"Date\"] <= train_end)\n",
    "    ]\n",
    "    \n",
    "    test = df_model[\n",
    "        (df_model[\"Date\"] >= test_start) &\n",
    "        (df_model[\"Date\"] <= test_end)\n",
    "    ]\n",
    "    \n",
    "    # Skip if no test data available\n",
    "    if len(test) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "    \n",
    "    X_test  = test[FEATURES]\n",
    "    y_test  = test[TARGET]\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Train the model\n",
    "    # ----------------------------\n",
    "    pipe = make_pipeline()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate predictions\n",
    "    # ----------------------------\n",
    "    y_prob = pipe.predict_proba(X_test)[:,1]  # Probability of positive class\n",
    "    y_pred = (y_prob > 0.5).astype(int)       # Binary predictions\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Evaluate model performance\n",
    "    # ----------------------------\n",
    "    auc  = roc_auc_score(y_test, y_prob)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    ll = log_loss(y_test, y_prob)\n",
    "    \n",
    "    # Store results for this fold\n",
    "    results.append({\n",
    "        \"train_start\": train_start,\n",
    "        \"train_end\": train_end,\n",
    "        \"test_start\": test_start,\n",
    "        \"test_end\": test_end,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"log_loss\": ll,\n",
    "        \"n_train\": len(train),\n",
    "        \"n_test\": len(test)\n",
    "    })\n",
    "    \n",
    "    # Print progress for each fold\n",
    "    print(\n",
    "        f\"{test_start} | \"\n",
    "        f\"AUC: {auc:.3f} | \"\n",
    "        f\"ACC: {acc:.3f} | \"\n",
    "        f\"Precision: {precision:.3f} | \"\n",
    "        f\"Recall: {recall:.3f} | \"\n",
    "        f\"F1: {f1:.3f} | \"\n",
    "        f\"Log Loss: {ll:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea8b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== OVERALL PERFORMANCE ====\n",
      "\n",
      "Mean AUC:  0.7706336624984312\n",
      "Mean ACC:  0.7039930924495074\n",
      "Mean Precision:  0.7016993688694898\n",
      "Mean Recall:  0.7129008572512185\n",
      "Mean F1:  0.7039794986299723\n",
      "Mean Log Loss:  0.5592356082286257\n",
      "\n",
      "By Year:\n",
      "           auc  accuracy\n",
      "year                    \n",
      "2022  0.776395  0.708288\n",
      "2023  0.764348  0.699308\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS: Overall and year-by-year performance\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n==== OVERALL PERFORMANCE ====\\n\")\n",
    "\n",
    "# Calculate mean metrics across all folds\n",
    "print(\"Mean AUC: \", results_df[\"auc\"].mean())\n",
    "print(\"Mean ACC: \", results_df[\"accuracy\"].mean())\n",
    "print(\"Mean Precision: \", results_df[\"precision\"].mean())\n",
    "print(\"Mean Recall: \", results_df[\"recall\"].mean())\n",
    "print(\"Mean F1: \", results_df[\"f1\"].mean())\n",
    "print(\"Mean Log Loss: \", results_df[\"log_loss\"].mean())\n",
    "\n",
    "print(\"\\nBy Year:\")\n",
    "\n",
    "# Extract year from test start date\n",
    "results_df[\"year\"] = results_df[\"test_start\"].dt.year\n",
    "\n",
    "# Show performance breakdown by year\n",
    "print(\n",
    "    results_df.groupby(\"year\")[[\"auc\",\"accuracy\"]].mean()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
